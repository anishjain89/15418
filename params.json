{
  "name": "Parallel Distributed Key-Value Store",
  "tagline": "CMU Spring 2016 15-418 project by Anish Jain(anishj) and Subodh Asthana(sasthana)",
  "body": "[SUMMARY](#RESOURCES)\r\n## **SUMMARY**\r\nTo design a parallel, scalable distributed key-value store which scales using consistent hashing on a cluster of Raspberry Pis.\r\n\r\n## **BACKGROUND**\r\nOur key-value store supports the following operations:\r\n* PUT: Insert/Update a new/existing key-value pair\r\n* GET: Retrieve the value associated with a given key\r\n* DEL: Remove a key-value pair from the database\r\n\r\nDistributing the database across several nodes provides multi-faceted advantages:\r\n* Allows us to run several PUT/GET/DEL operations in parallel\r\n* Storage load is amortized across multiple nodes\r\n\r\nThe basic architecture consists of a central coordinator and several storage nodes. The clients send requests to the coordinator which routes the requests to appropriate storage nodes. Request routing decisions are taken based on a consistent-hashing ring. The central coordinator has information about all the storage nodes present in the system. Based on the storage load on individual storage nodes, it takes scaling decisions where addition/deletion of storage nodes is handled using consistent hashing. Migration of data between storage nodes during scaling is handled in the background so that there is no downtime. Storage nodes store the key-value pairs assigned to it in-memory and service requests routed to it. We'll be programming our project in Go as it enables parallelism via low-cost goroutines. The GoRPC package spawns a new goroutine for each request which enables parallelism across multiple requests. \r\n\r\n## **CHALLENGES**\r\nA few challenges that we foresee:\r\n* Setting up the cluster of Raspberry Pis: We need to understand how basic CPU usage + networking of the Raspberry Pis work, and also need to make sure that they can talk to each other on a local network. For our prototyping, we'll be connecting the Pis on a local network using switch/hub. \r\n* Setting up the software framework so that all the nodes in the system have required communication channels set up.\r\n* Correctly implementing consistent hashing: This is the crux of our project. We need to ensure that every request is being sent to the appropriate storage node based on it's key following the correct semantics of consistent hashing. We also need to ensure that during scale-in/scale-out data migration based on keys is correct.\r\n\r\n## **RESOURCES**\r\n* Hardware\r\n  * Five Type B 512 MB RAM Raspberry Pis with 8GB flash storage\r\n  * TP-link 8 port networking switch\r\n  * A whole lot of ethernet cables\r\n* Software\r\n  * Golang, with a focus on the RPC package\r\n\r\n## **GOALS AND DELIVERABLES**\r\n* A basic system with all the communication channels set up between the nodes:\r\n\r\n  We want to build a basic system with storage nodes labelled such that they follow the semantics of consistent hashing. We will need to prototype and test the RPC functionality by passing known objects between different nodes in the system.\r\n* Prototyping scale-out/scale-in:\r\n\r\n  Fine-tune parameters to find out the correct mix of nodes for different workloads. Ensure that the addition/deletion and migration of data is correct as per the consistent hashing semantics. Profile the behavior of Pis as the in-memory hash tables grow big.\r\n\r\n### ADD-ON GOALS\r\nA few addons that we want to explore if time permits:\r\n* Replication of data across multiple storage nodes for basic fault tolerance/improved throughput using PAXOS or 2 phase commits\r\n* Optimizing the system for large \"value\" objects like images, audio files\r\n  * Replicating the functionality of the single coordinator\r\n* Persisting the in-memory key-value store on flash to make the data durable\r\n* Comparing the performance of C/C++ implementation of the same design vs Go to get an idea of efficiency of touted go routines vs worker threads\r\n\r\n### PLANNED DEMO\r\nWe hope to demo a working model of this proposed small-scale key-value architecture. We would demonstrate all the features guaranteed by our system namely put, get, delete, node join and node exit. We also hope to compare the performance of this distributed key-value store against a single node version of it.\r\n\r\n### PERFORMANCE\r\n* As of now it would be difficult for us to predict the performance specifics because:\r\n  * We are new to Raspberry Pis and therefore we would need to profile the system with our workloads.\r\n  * We think that the performance of the system will change as we increase the size of \"values\" associated with each key.\r\n* We hope that the performance of our N storage nodes version of distributed key-value store to be at least K times better than the single node version where K <= N\r\n\r\n## **PLATFORM CHOICE**\r\nRaspberry Pis are cheap and power efficient and provides fast access to persistent storage (flash vs disks). Pis are readily accessible and therefore we hope that our project can be used by other students to play around with simple distributed systems for academic purposes. Golang is used as it's an effective systems programming language to design thread-safe efficient parallel distributed systems.\r\n\r\n## **SCHEDULE**\r\n* April 4 - April 10: Setting up the cluster (hardware and software). Familiarizing ourselves with GoLang.\r\n* April 11 - April 17: Implementing communication framework. Design software APIs. Read up on consistent hashing. \r\n* April 18 - April 24: Implement correct version of consistent hashing. Come up with workloads to test the system.\r\n* April 25 - May 1: Implement scaling with data migration. Fine tune scaling parameters.\r\n* May 2 - May 8: Make the system robust. Work on future goals. Make final presentation. Demo!!!\r\n\r\n## **CHECKPOINT REPORT**\r\n###SUMMARY\r\n***\r\nSo far we have achieved the following:\r\n\r\n1)\tWe have started the project by reading upon consistent hashing. We tried to understand how it works, what are its advantages and found out what are some simple ideas that we can use to implement in our project.\r\n\r\n2)\tWe are new to GoLang therefore we read up on that and few presentations about it. We wrote few simple programs initially to get used to GoLang.\r\n\r\n3)\tWe have written simple programs in GoLang to test if we are able to communicate between nodes. For now our communication assumes that the IP addresses of the nodes participating in communication are known already. We plan to build node discovery framework so that in case of dynamic IP addresses we are able to communicate with the devices. We still will assume that the IP address of the master node is known to all the slave nodes.\r\n\r\n4)\tHaving done this we also experimented with the goRPC package to transfer data between the nodes. We plan to use goRPC extensively in our project for GET/PUT requests therefore getting it to work correctly was crucial.\r\n\r\n###MAJOR CONCERNS\r\n***\r\nWe don’t have access to Raspberry Pis yet and most of our development is being done using the GHC machines. We will still need some time to setup the cluster. Particularly, we think that most of our time while setting up the cluster will be spent on installing and bringing up the Go compiler assuming that we won’t face much trouble with the default unix distribution that comes with Pi.\r\nWe are still deciding what will be the best type of value object that we want to use to demo our key-value store so that we can highlight the benefits of Raspberry Pis, if there are any. For now we are considering either small String phrases or 1 MB image files. We plan to experiment with both kind of value objects and see how our system performs. \r\n\r\n###OLD GOALS\r\n***\r\nOur proposed goals were to implement a Key-Value store with GET, PUT and DEL API support. We were hoping to demonstrate the dynamic scale-in/scale-out of the nodes using consistent hashing as the load on the system increases or decreases. This load can be described along two axes:\r\n\r\n\r\n1)\tMemory overhead on a single data node\r\n\r\n2)\tNumber of requests that are seen by a single data node\r\n\r\nAs we scale-in/scale-out the cluster of Pis we were planning to compare its perf/watt and perf/dollar to a regular commercial machine.\r\n\r\n###NEW GOALS\r\n***\r\nOur goals for the project are pretty much the same as we had initially proposed. In context of what would be feasible to demo on the project presentation day we think we will demonstrate scale-out of the cluster. We plan to make nodes Join the cluster as the load on a single data node increases so that performance comparable to (or some factor of) commercial machines can be achieved. Therefore we hope to demo node addition and data migration among the nodes as one of the tests.  Apart from that we plan to show graph of average response times as the number of clients grow, performance/watt and performance/dollar. For the performance/watt we, for now, plan to use the power numbers that are mentioned in the hardware specification document of the concerned machines that we are using. One of the things that we had planned initially to demonstrate but might not do it for the final presentation is to demonstrate the scale-in of the cluster. This is primarily because as of now we are not sure what heuristics to use to scale-in the cluster. If we are able to come up with a good heuristic and test in time then we can certainly demonstrate that as well.\r\nTo perform a live demo we need to go forward with the assumption that the IP address of the master node is known to all the slave nodes. This is one addition to our initial goals that we hope to implement a very basic slave node discovery protocol which will run on the master. We don’t know if we will be to implement this given the amount of time that we have but as a fallback we will be using static IP address or a configuration file which will contain the IP address of all the nodes so that the communication between nodes is easy.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}