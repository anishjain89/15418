{
  "name": "Parallel Distributed Key-Value Store",
  "tagline": "CMU Spring 2016 15-418 project by Anish Jain(anishj) and Subodh Asthana(sasthana)",
  "body": "## SUMMARY\r\nTo design a parallel, scalable distributed key-value store which scales using consistent hashing on a cluster of Raspberry Pis.\r\n\r\n## BACKGROUND\r\nOur key-value store supports the following operations:\r\n* PUT: Insert/Update a new/existing key-value pair\r\n* GET: Retrieve the value associated with a given key\r\n* DEL: Remove a key-value pair from the database\r\n\r\nDistributing the database across several nodes provides multi-faceted advantages:\r\n* Allows us to run several PUT/GET operations in parallel\r\n* Storage load is amortized across multiple nodes\r\n\r\nThe basic architecture consists of a central coordinator and several storage nodes. The clients send requests to the coordinator which routes the requests to appropriate storage nodes. Request routing decisions are taken based on a consistent-hashing ring. The central coordinator has information about all the storage nodes present in the system. Based on the storage load on individual storage nodes, it takes scaling decisions where addition/deletion of storage nodes is handled using consistent hashing. Migration of data between storage nodes during scaling is handled in the background so that there is no downtime. Storage nodes store the key-value pairs assigned to it in-memory and service requests routed to it. We'll be programming our project in Go as it enables parallelism via low-cost goroutines. The GoRPC package spawns a new goroutine for each request which enables parallelism across multiple requests. \r\n\r\n## CHALLENGES\r\nA few challenges that we foresee:\r\n* Setting up the cluster of Raspberry Pis: We need to understand how basic CPU usage + networking of the Raspberry Pis work, and also need to make sure that they can talk to each other on a local network. For our prototyping, we'll be connecting the Pis on a local network using switch/hub. \r\n* Setting up the software framework so that all the nodes in the system have required communication channels set up.\r\n* Correctly implementing consistent hashing: This is the crux of our project. We need to ensure that every request is being sent to the appropriate storage node based on it's key following the correct semantics of consistent hashing. We also need to ensure that during scale-in/scale-out data migration based on keys is correct.\r\n\r\n## RESOURCES\r\n* Hardware\r\n  * Five Type B 512 MB RAM Raspberry Pis with 8GB flash storage\r\n  * TP-link 8 port networking switch\r\n  * A whole lot of ethernet cables\r\n* Software\r\n  * Golang, with a focus on the RPC package\r\n\r\n## GOALS AND DELIVERABLES\r\n* A basic system with all the communication channels set up between the nodes\r\n  We want to build a basic system which with storage nodes labelled such that they follow the semantics of consistent hashing. We will need to prototype and test the RPC functionality by passing known objects between different nodes in the system.\r\n* Prototyping scale-out/scale-in\r\n  Fine-tune parameters to find out the correct mix of nodes for different workloads. Ensure that the addition/deletion and migration of data is correct as per the consistent hashing semantics. Profile the behavior of Pis as the in-memory hash tables grow big.\r\n\r\n### ADD-ON GOALS\r\nA few addons that we want to explore if time permits:\r\n* Replication of data across multiple storage nodes for basic fault tolerance/improved throughput using PAXOS or 2 phase commits\r\n* Optimizing the system for large \"value\" objects like images, audio files\r\n  * Replicating the functionality of the single coordinator\r\n* Persisting the in-memory key-value store on flash to make the data durable\r\n* Comparing the performance of C/C++ implementation of the same design vs Go to get an idea of efficiency of touted go routines vs worker threads\r\n\r\n### PLANNED DEMO\r\nWe hope to demo a working model of this proposed small-scale key-value architecture. We would demonstrate all the features guaranteed by our system namely put, get, delete, node join, node exit. We also hope to compare the performance of this distributed key-value store against a single node version of it.\r\n\r\n### PERFORMANCE\r\n* As of now it would be difficult for us to predict the performance specifics because:\r\n  * We are new to Raspberry Pis and therefore we would need to profile the system with our workloads.\r\n  * We think that the performance of the system will change as we increase the size of \"values\" associated with each key.\r\n* We hope that the performance of our N storage nodes version of distributed key-value store to be at least K times better than the single node version where K <= N\r\n\r\n## PLATFORM CHOICE\r\nRaspberry Pis are cheap and power efficient and provides fast access to persistent storage (flash vs disks). Pis are readily accessible and therefore we hope that our project can be used by other students to play around with simple distributed systems for academic purposes. Golang is used as it's an effective systems programming language to design thread-safe efficient parallel distributed systems.\r\n\r\n## SCHEDULE\r\n* April 4 - April 10 - Setting up the cluster (hardware and software). Familiarizing ourselves with GoLang.\r\n* April 11 - April 17 - Implementing communication framework. Design software APIs. Read up on consistent hashing. \r\n* April 18 - April 24 - Implement correct version of consistent hashing. Come up with workloads to test the system.\r\n* April 25 - May 1 - Implement scaling with data migration. Fine tune scaling parameters.\r\n* May 2 - May 8 - Make the system robust. Work on future goals. Make final presentation. Demo!!!",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}