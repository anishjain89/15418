{
  "name": "Parallel Distributed Key Value Store",
  "tagline": "A 15-418 project by Anish Jain(anishj) and Subodh Asthana(sasthana)",
  "body": "### SUMMARY\r\nTo design a parallel, scalable distributed key-value store which scales using consistent hashing on a cluster of Raspberry Pis\r\n\r\n### BACKGROUND\r\nOur key-value store supports the following operations:\r\n* PUT: Insert/Update a new/existing key-value pair\r\n* GET: Retrieve the value associated with a given key\r\n\r\nDistributing the database across several nodes provides multi-faceted advantages:\r\n* Allows us to run several PUT/GET operations in parallel\r\n* Storage load is amortized across multiple nodes\r\n\r\nThe basic architecture consists of a central coordinator and several storage nodes. The clients send requests to the coordinator which routes the requests to appropriate storage nodes. Request routing decisions are taken based on a consistent-hashing ring. The central coordinator has information about all the storage nodes present in the system. Based on the observed load, it takes scaling decisions where addition/deletion of storage nodes is handled using consistent hashing. Migration of data between storage nodes during scaling is handled in the background as soft updates so that there is no downtime. Storage nodes store the key-value pairs assigned to it in-memory and service requests. We'll be programming in Go as it enables parallelism via low-cost goroutines. The GoRPC package spawns a new goroutine for each request which enables parallelism across multiple requests. \r\n\r\n### CHALLENGE\r\nA few challenges that we foresee:\r\n* Setting up the cluster of Raspberry Pis: We need to understand how basic CPU usage + networking of the Raspberry Pis work, and also need to make sure that they can talk to each other on a local network. For our prototyping, we'll be connecting the Pis on a local network using switch/hub. \r\n* Setting up the software framework so that all the nodes in the system have required communication channels set up.\r\n* Correctly implementing consistent hashing: This is the crux of our project. We need to ensure that every request is being sent to the appropriate storage node based on it's key following the correct semantics of consistent hashing. We also need to ensure during scale-in/scale-out data migration based on keys are correct.\r\nA few addons that we want to explore if time permits:\r\n* Replication of data across multiple storage nodes for basic fault tolerance/improved throughput using PAXOS or 2 phase commits\r\n* Optimizing the system for large \"value\" objects like images, audio files\r\n  * Replicating the functionality of the single coordinator\r\n* Persisting the in-memory key-value store on flash to make the data durable\r\n* Comparing the performance of C/C++ implementation of the same design vs Go to get an idea of efficiency of touted go routines vs worker threads\r\n\r\n### RESOURCES\r\n* Hardware\r\n  * Five Type B 512 MB RAM Raspberry Pis with 8GB flash storage\r\n  * TP-link 8 port networking switch\r\n  * A whole lot of ethernet cables\r\n* Software\r\n  * Golang, with a focus on the RPC package\r\n\r\n### PLATFORM CHOICE\r\n\r\n\r\n### GOALS AND DELIVERABLES\r\n\r\n### SCHEDULE\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}